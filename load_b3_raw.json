{
	"jobConfig": {
		"name": "load_b3_raw",
		"description": "",
		"role": "arn:aws:iam::551980418343:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 5,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "load_b3_raw.py",
		"scriptLocation": "s3://aws-glue-assets-551980418343-sa-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-15T23:03:45.302Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-551980418343-sa-east-1/temporary/",
		"additionalPythonModules": "yfinance",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-551980418343-sa-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "\"\"\"\r\nGlue Job – RAW | Ingestão de dados da B3 via Yahoo Finance\r\n\r\nResponsabilidades:\r\n- Extrair dados diários de ações/índices\r\n- Persistir dados brutos em S3 (camada raw)\r\n- Gerar sinal (_READY.json) para orquestração downstream via Lambda\r\n\r\nObservação:\r\nA data processada corresponde ao último pregão disponível retornado\r\npela API do Yahoo Finance no momento da execução.\r\n\"\"\"\r\n\r\nimport sys\r\nimport json\r\nimport boto3\r\nimport yfinance as yf\r\nimport pandas as pd\r\nfrom datetime import datetime\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.utils import getResolvedOptions\r\n\r\n# =========================\r\n# Inicialização Glue / Spark\r\n# =========================\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Permite sobrescrever apenas a partição impactada\r\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n\r\n# =========================\r\n# Configurações\r\n# =========================\r\nBUCKET_NAME = \"fiap-b3-data\"\r\nS3_PATH = f\"s3a://{BUCKET_NAME}/raw/b3_stock/\"\r\nTICKERS = [\"PETR4.SA\", \"VALE3.SA\", \"B3SA3.SA\", \"^BVSP\"]\r\n\r\nprint(f\"Iniciando extração para os tickers: {TICKERS}\")\r\n\r\n# =========================\r\n# Extração\r\n# =========================\r\ndf_raw = yf.download(\r\n    TICKERS,\r\n    period=\"7d\",     # margem para garantir último pregão\r\n    interval=\"1d\",\r\n    progress=False\r\n)\r\n\r\nif df_raw.empty:\r\n    print(\"❌ Nenhum dado retornado pelo Yahoo Finance\")\r\n    job.commit()\r\n    sys.exit(0)\r\n\r\n# =========================\r\n# Transformação Pandas\r\n# =========================\r\ndf_long = df_raw.stack(level=1).reset_index()\r\ndf_long.columns = [c.lower().replace(\" \", \"_\") for c in df_long.columns]\r\ndf_long.rename(columns={'level_1': 'ticker'}, inplace=True)\r\n\r\n# Seleciona apenas o último pregão disponível\r\nmax_date = df_long['date'].max()\r\ndf_last_day = df_long[df_long['date'] == max_date].copy()\r\n\r\n# Criação da partição\r\ndf_last_day['dt'] = df_last_day['date'].dt.strftime('%Y-%m-%d')\r\npartition_val = df_last_day['dt'].iloc[0]\r\n\r\n# Normalização dos tickers\r\ndf_last_day['ticker'] = (\r\n    df_last_day['ticker']\r\n    .str.replace(r'[\\^]', '', regex=True)\r\n    .str.replace('.SA', '', regex=False)\r\n)\r\n\r\n# =========================\r\n# Conversão para Spark\r\n# =========================\r\ndf_last_day['date'] = df_last_day['date'].astype(str)\r\nspark_df = spark.createDataFrame(df_last_day)\r\n\r\n# =========================\r\n# Escrita RAW\r\n# =========================\r\n(\r\n    spark_df\r\n    .coalesce(1)   # volume pequeno (desafio)\r\n    .write\r\n    .mode(\"overwrite\")\r\n    .partitionBy(\"dt\")\r\n    .parquet(S3_PATH)\r\n)\r\n\r\n# =========================\r\n# Sinal de conclusão (_READY)\r\n# =========================\r\ns3 = boto3.client(\"s3\")\r\n\r\nready_payload = {\r\n    \"process_date\": partition_val,\r\n    \"dataset\": \"b3_stock\",\r\n    \"generated_at\": datetime.utcnow().isoformat()\r\n}\r\n\r\ns3.put_object(\r\n    Bucket=BUCKET_NAME,\r\n    Key=\"raw/b3_stock/_READY.json\",\r\n    Body=json.dumps(ready_payload)\r\n)\r\n\r\nprint(f\"✅ RAW finalizada para dt={partition_val}\")\r\njob.commit()\r\n"
}