{
	"jobConfig": {
		"name": "etl_b3_refined",
		"description": "",
		"role": "arn:aws:iam::551980418343:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 10,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "etl_b3_refined.py",
		"scriptLocation": "s3://aws-glue-assets-551980418343-sa-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-15T02:28:15.352Z",
		"developerMode": true,
		"connectionsList": [
			"Network connection"
		],
		"temporaryDirectory": "s3://aws-glue-assets-551980418343-sa-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-551980418343-sa-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom datetime import datetime, timedelta\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.window import Window\r\n\r\n# =========================\r\n# INIT\r\n# =========================\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"PROCESS_DATE\"])\r\nprocess_date = args[\"PROCESS_DATE\"] \r\n\r\nprint(f\"PROCESS_DATE recebido: {process_date}\")\r\n\r\nprocess_dt = datetime.strptime(process_date, \"%Y-%m-%d\")\r\nstart_dt = (process_dt - timedelta(days=5)).strftime(\"%Y-%m-%d\")\r\n\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\n\r\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n\r\nBUCKET = \"fiap-b3-data\"\r\nraw_path = f\"s3a://{BUCKET}/raw/b3_stock/\"\r\nrefined_path = f\"s3a://{BUCKET}/refined/b3_stock/\"\r\n\r\n# =========================\r\n# READ RAW \r\n# =========================\r\ndf_raw = spark.read.parquet(raw_path)\r\n\r\ndf = (\r\n    df_raw\r\n    .filter(\r\n        (F.col(\"dt\") >= start_dt) &\r\n        (F.col(\"dt\") <= process_date)\r\n    )\r\n)\r\n\r\nprint(f\"Processando janela {start_dt} -> {process_date}\")\r\nprint(\"RAW COUNT:\", df.count())\r\n\r\n# =========================\r\n# TRANSFORMAÇÃO agrupamento por ticket, data e alteraçào de nomes das colunas\r\n# =========================\r\ndf_agg = (\r\n    df.groupBy(\"ticker\", \"dt\")\r\n      .agg(\r\n          F.avg(\"close\").alias(\"avg_close\"),\r\n          F.sum(\"volume\").alias(\"total_volume\")\r\n      )\r\n)\r\n\r\n# =========================\r\n# TRANSFORMAÇÃO reanem colunas\r\n# =========================\r\ndf_renamed = (\r\n    df_agg\r\n    .withColumnRenamed(\"avg_close\", \"media_fechamento\")\r\n    .withColumnRenamed(\"total_volume\", \"volume_total\")\r\n)\r\n\r\n# =========================\r\n# TRANSFORMAÇÃO busca ultimos 5 dias apra fazer o avg do fechamento\r\n# =========================\r\nwindow_spec = (\r\n    Window\r\n    .partitionBy(\"ticker\")\r\n    .orderBy(\"dt\")\r\n    .rowsBetween(-4, 0)\r\n)\r\n\r\ndf_final = df_renamed.withColumn(\r\n    \"media_movel_5d\",\r\n    F.avg(\"media_fechamento\").over(window_spec)\r\n)\r\n\r\n# Mantém apenas o dia processado na escrita\r\ndf_out = df_final.filter(F.col(\"dt\") == process_date)\r\n\r\nprint(\"FINAL COUNT:\", df_out.count())\r\n\r\n# =========================\r\n# WRITE REFINED\r\n# =========================\r\n(\r\n    df_out\r\n    .write\r\n    .mode(\"overwrite\")\r\n    .partitionBy(\"dt\", \"ticker\")\r\n    .parquet(refined_path)\r\n)\r\n\r\njob.commit()\r\n"
}